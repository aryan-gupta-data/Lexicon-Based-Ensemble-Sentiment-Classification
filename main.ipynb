{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP551-Project.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aryan-gupta-data/Sentiment-Analysis/blob/master/COMP551_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zh8j-LoxD0qE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Mount Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "vxCXsJzbwnpd",
        "colab_type": "code",
        "outputId": "b51442c6-b418-4a76-bdcc-62f474ec6ba6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "## try running second cell frist. if doesnt work run this cell and try again.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oat53kArDwHq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Package **Imports**"
      ]
    },
    {
      "metadata": {
        "id": "SPrQBWQ4gaY2",
        "colab_type": "code",
        "outputId": "da8bcd7d-c0d7-4cce-ef36-940a0749c4c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "#!pip install nltk  # in case corrector needs to install package\n",
        "\n",
        "import gzip\n",
        "import string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics import classification_report, accuracy_score, recall_score, precision_score, f1_score\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn import svm, tree\n",
        "from numpy import random\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import numpy as np\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "from scipy.stats import variation\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from scipy.sparse import spmatrix as spm"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BlTjKy9ASaaE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Import Functions"
      ]
    },
    {
      "metadata": {
        "id": "jRqrdkY9PO5Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# script to parse comments from original text files. detects reviews and ratings, and saves into separate variables\n",
        "\n",
        "def parse(filename):\n",
        "  f = gzip.open(filename, \"rt\", encoding=\"utf-8\")\n",
        "  revs=[]\n",
        "  rates=[]\n",
        "  counter=0\n",
        "\n",
        "  for l in f:\n",
        "    #counter = counter + 1\n",
        "    rev_flag = l.find('review/text')\n",
        "    if rev_flag == 0:\n",
        "      rev = l[13:-1]\n",
        "      revs.append(rev)\n",
        "    rate_flag = l.find('review/score')\n",
        "    if rate_flag == 0:\n",
        "      rate = l[14:-1]\n",
        "      rates.append(rate)\n",
        "  f.close()\n",
        "  return revs, rates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ECqooiiPVyB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reads original datasets, splits them into separate files based on rating\n",
        "\n",
        "def saveData(filename):\n",
        "    print('Parsing file...')\n",
        "    cat_revs, cat_rates = parse(filename)  # run function on auto data\n",
        "\n",
        "    print('Writing files...')\n",
        "\n",
        "    myfile1 = open('/content/drive/My Drive/COMP551/Final Project/input/health_ratings_1.txt', 'w')\n",
        "    myfile2 = open('/content/drive/My Drive/COMP551/Final Project/input/health_ratings_2.txt', 'w')\n",
        "    myfile3 = open('/content/drive/My Drive/COMP551/Final Project/input/health_ratings_3.txt', 'w')\n",
        "    myfile4 = open('/content/drive/My Drive/COMP551/Final Project/input/health_ratings_4.txt', 'w')\n",
        "    myfile5 = open('/content/drive/My Drive/COMP551/Final Project/input/health_ratings_5.txt', 'w')\n",
        "    for i, rate in enumerate(cat_rates):\n",
        "        if rate == '1.0':\n",
        "            myfile1.write('%s\\n' % cat_revs[i])\n",
        "        elif rate == '2.0':\n",
        "            myfile2.write('%s\\n' % cat_revs[i])\n",
        "        elif rate == '3.0':\n",
        "            myfile3.write('%s\\n' % cat_revs[i])\n",
        "        elif rate == '4.0':\n",
        "            myfile4.write('%s\\n' % cat_revs[i])\n",
        "        elif rate == '5.0':\n",
        "            myfile5.write('%s\\n' % cat_revs[i])\n",
        "\n",
        "    myfile1.close()\n",
        "    myfile2.close()\n",
        "    myfile3.close()\n",
        "    myfile4.close()\n",
        "    myfile5.close()\n",
        "\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ff06l8brNAUK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reads rating-split dataset, removes reviews less than 100 characters, shuffles, writes to new files with specified number of reviews\n",
        "\n",
        "def filterData(filename, count):\n",
        "\n",
        "    file = open(filename, 'r')\n",
        "    lines = file.readlines()\n",
        "\n",
        "    lines_filtered = []\n",
        "    for line in lines:\n",
        "        if len(line) > 100:\n",
        "            lines_filtered.append(line)\n",
        "\n",
        "    lines = random.sample(lines_filtered, count)\n",
        "\n",
        "    filename, file_ext = os.path.splitext(filename)\n",
        "    filename_new = filename + '_filtered' + file_ext\n",
        "\n",
        "    with open(filename_new, 'w') as f:\n",
        "        for line in lines:\n",
        "            f.write(\"%s\" % line)\n",
        "\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EAQ8ZsCSMndm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Reads filtered datasets, combines ones of the same category, adds ratings to end of reviews\n",
        "\n",
        "def reconstructData(filename_new, filenames, ratings):\n",
        "    with open(filename_new, 'w') as f:\n",
        "        for filename, rating in zip(filenames, ratings):\n",
        "            with open(filename, 'r') as file:\n",
        "                lines = file.readlines()\n",
        "                for line in lines:\n",
        "                    f.write(\"%s\\t%d\\n\" % (line.rstrip(), rating))\n",
        "\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pv332gLLMqPZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Read from specified file, recategorizes to 'negative', 'neutral', or 'positive', returns training x and y sets\n",
        "\n",
        "def createDataset(train_file):\n",
        "    with open(train_file, 'r') as file:\n",
        "        train = file.readlines()\n",
        "\n",
        "    # get targets\n",
        "    y_train = list(map(lambda x: int(x.strip()[-1]), train))\n",
        "    y_train = np.asarray(y_train)\n",
        "\n",
        "    # remove targets\n",
        "    x_train = list(map(lambda x: x.strip().rstrip('12345').strip(), train))\n",
        "    x_train = np.asarray(x_train)\n",
        "\n",
        "    idx_1 = np.asarray(np.where(y_train == 1)).ravel()\n",
        "    idx_2 = np.asarray(np.where(y_train == 2)).ravel()\n",
        "    idx_3 = np.asarray(np.where(y_train == 3)).ravel()\n",
        "    idx_4 = np.asarray(np.where(y_train == 4)).ravel()\n",
        "    idx_5 = np.asarray(np.where(y_train == 5)).ravel()\n",
        "\n",
        "    idx_neg = np.concatenate((idx_1, idx_2), axis=None)\n",
        "    idx_neu = idx_3\n",
        "    idx_pos = np.concatenate((idx_4, idx_5), axis=None)\n",
        "\n",
        "    x_train_neg = x_train[idx_neg]\n",
        "    y_train_neg = np.full(len(idx_neg), \"negative\")\n",
        "\n",
        "    x_train_neu = x_train[idx_neu]\n",
        "    y_train_neu = np.full(len(idx_neu), \"neutral\")\n",
        "\n",
        "    x_train_pos = x_train[idx_pos]\n",
        "    y_train_pos = np.full(len(idx_pos), \"positive\")\n",
        "\n",
        "    x_train = np.concatenate((np.reshape(x_train_neg, (len(x_train_neg), 1)),\n",
        "                              np.reshape(x_train_neu, (len(x_train_neu), 1)),\n",
        "                              np.reshape(x_train_pos, (len(x_train_pos), 1))),\n",
        "                             axis=0)\n",
        "\n",
        "    y_train = np.concatenate((np.reshape(y_train_neg, (len(y_train_neg), 1)),\n",
        "                              np.reshape(y_train_neu, (len(y_train_neu), 1)),\n",
        "                              np.reshape(y_train_pos, (len(y_train_pos), 1))),\n",
        "                             axis=0)\n",
        "\n",
        "    return x_train, y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GdPVGRczEE2B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Preprocessing Function"
      ]
    },
    {
      "metadata": {
        "id": "hlzGClCcnfVg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# use it like this..... x_train = text_preprocess(x_train)\n",
        "\n",
        "def text_preprocess(data):\n",
        "    data = np.array(data)\n",
        "    data = data#[0]  # just take portion for debugging\n",
        "    for i in range(len(data)): \n",
        "        temp_str = data[i][0]\n",
        "        \n",
        "        temp_str = temp_str.lower()                 # Converting to lowercase\n",
        "        cleanr = re.compile('<.*?>')\n",
        "        temp_str = re.sub(cleanr, ' ', temp_str)        #Removing HTML tags\n",
        "        temp_str = re.sub(r'[?|!||\"|#|,|.|:|/]',r' ',temp_str)\n",
        "        temp_str = re.sub(r'[\\'|\\-|*)|(||/]',r'',temp_str)        #Removing Punctuations excepy ',' and '.'\n",
        "        \n",
        "        data[i][0] = temp_str \n",
        "    \n",
        "    data = list(data)\n",
        "    #Lemming\n",
        "    lemmer=WordNetLemmatizer()\n",
        "    data_preprocessed = data\n",
        "    auto_revs_lemmered = []\n",
        "    for i in range(len(data_preprocessed)):\n",
        "        temp_sentence = data_preprocessed[i][0].replace(\",\", \" \") # Replace ',' by space\n",
        "        temp_sentence = temp_sentence.replace(\".\", \" \") #Replace '.' by space\n",
        "        temp_sentence_lemmered=[' '.join([lemmer.lemmatize(temp_sentence_words, 'v') for temp_sentence_words in temp_sentence.split(' ')])]\n",
        "        temp_sentence_lemmered=[' '.join([lemmer.lemmatize(temp_sentence_words, 'a') for temp_sentence_words in temp_sentence_lemmered[0].split(' ')])]\n",
        "        auto_revs_lemmered.append((temp_sentence_lemmered[0]))\n",
        "    \n",
        "    return np.array(auto_revs_lemmered)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5VyZgrjHSFnb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Baseline Reproduction"
      ]
    },
    {
      "metadata": {
        "id": "7v8-Lq7-OpRB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Baseline reproduction Decision Tree classification\n",
        "\n",
        "filename_train = \"drive/My Drive/COMP551-Project/automotive_filtered.txt\"\n",
        "x_train_movies, y_train_movies = createDataset(filename_train)\n",
        "\n",
        "v_bin = CountVectorizer(analyzer=\"word\", binary=True, token_pattern=u\"(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b\")\n",
        "x_data = v_bin.fit_transform(x_train_movies.ravel())\n",
        "y_data = y_train_movies\n",
        "\n",
        "dt = tree.DecisionTreeClassifier()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E4lAZW5CIQxf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params = {'criterion': ['entropy'],\n",
        "          'max_depth': [20, 40, 60, 80, 100],\n",
        "          'min_samples_split': [80, 100, 120, 140, 160, 180, 200, 220]}\n",
        "clf = GridSearchCV(dt, params, cv=10, scoring='f1_macro')\n",
        "x_train = x_data\n",
        "y_train = y_data\n",
        "clf.fit(x_train, y_train)\n",
        "print('File: ', filename_train)\n",
        "print('Best Param: ', clf.best_params_)\n",
        "\n",
        "means = clf.cv_results_['mean_test_score']\n",
        "stds = clf.cv_results_['std_test_score']\n",
        "\n",
        "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
        "    print(\"%0.3f (+/-%0.03f) for %r\"\n",
        "          % (mean, std * 2, params))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_nce3QJP4ZBJ",
        "colab_type": "code",
        "outputId": "c20f8aab-ddf6-444c-c1eb-e1bc54e43d15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True)\n",
        "for train_index, test_index in kf.split(x_data, y_data):\n",
        "    x_train, x_test = x_data[train_index], x_data[test_index]\n",
        "    y_train, y_test = y_data[train_index], y_data[test_index]\n",
        "\n",
        "    dt = dt.fit(x_train, y_train)\n",
        "    y_pred = dt.predict(x_test)\n",
        "\n",
        "    accuracy_scores.append(accuracy_score(y_test, y_pred))\n",
        "    precision_scores.append(precision_score(y_test, y_pred, average='macro'))\n",
        "    recall_scores.append(recall_score(y_test, y_pred, average='macro'))\n",
        "    f1_scores.append(f1_score(y_test, y_pred, average='macro'))\n",
        "\n",
        "print(\"Accuracy: %0.3f (+/- %0.3f)\" % (np.mean(accuracy_scores), np.std(accuracy_scores) * 2))\n",
        "print(\"Precision: %0.3f (+/- %0.3f)\" % (np.mean(precision_scores), np.std(precision_scores) * 2))\n",
        "print(\"Recall: %0.3f (+/- %0.3f)\" % (np.mean(recall_scores), np.std(recall_scores) * 2))\n",
        "print(\"F-measure: %0.3f (+/- %0.3f)\" % (np.mean(f1_scores), np.std(f1_scores) * 2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.506 (+/- 0.039)\n",
            "Precision: 0.506 (+/- 0.039)\n",
            "Recall: 0.506 (+/- 0.041)\n",
            "F-measure: 0.506 (+/- 0.040)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HmLMBN9AevFX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filename_train = \"/content/drive/My Drive/COMP551/Final Project/input/movies_filtered.txt\"\n",
        "x_train_movies, y_train_movies = createDataset(filename_train)\n",
        "\n",
        "v_bin = CountVectorizer(analyzer=\"word\", binary=True, token_pattern=u\"(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b\")\n",
        "x_data = v_bin.fit_transform(x_train_movies.ravel())\n",
        "y_data = y_train_movies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a9k7RvD4ZwRm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "n9zLYjl7eyTV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print (filename_train[1:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcqt5AP1ZueP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "filename_train \n",
        "\n",
        "Type - str\n",
        "<br>len - 71"
      ]
    },
    {
      "metadata": {
        "id": "VGKbeh3SDj-K",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Extraction and Preprocessing"
      ]
    },
    {
      "metadata": {
        "id": "8UCGHx3WryH0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#auto_revs_og, auto_rates_og = parse(\"drive/My Drive/COMP551-Project/Automotive.txt.gz\")\n",
        "auto_revs_og, auto_rates = createDataset(\"drive/My Drive/COMP551-Project/automotive_filtered.txt\")\n",
        "auto_revs = text_preprocess(auto_revs_og)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z_34IWpoN9vw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# import pandas as pd                                 #for data manipulation and analysis\n",
        "# import nltk                                         #Natural language processing tool-kit\n",
        "# from nltk.corpus import stopwords                   #Stopwords corpus\n",
        "# from nltk.stem import PorterStemmer \n",
        "# import re\n",
        "# from nltk.stem.snowball import SnowballStemmer\n",
        "# from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# def text_preprocess(data):\n",
        "#     data = np.array(data)\n",
        "#     data = data#[0]  # just take portion for debugging\n",
        "#     for i in range(len(data)): \n",
        "#         data[i] = data[i].lower()                 # Converting to lowercase\n",
        "#         cleanr = re.compile('<.*?>')\n",
        "#         data[i] = re.sub(cleanr, ' ', data[i])        #Removing HTML tags\n",
        "#         data[i] = re.sub(r'[?|!||\"|#|,|.|:|/]',r' ',data[i])\n",
        "#         data[i] = re.sub(r'[\\'|\\-|*)|(||/]',r'',data[i])        #Removing Punctuations excepy ',' and '.'\n",
        "#     data = list(data)\n",
        "#     #Lemming\n",
        "#     lemmer=WordNetLemmatizer()\n",
        "#     data_preprocessed = data\n",
        "#     auto_revs_lemmered = []\n",
        "#     for i in range(len(data_preprocessed)):\n",
        "#         temp_sentence = data_preprocessed[i].replace(\",\", \" \") # Replace ',' by space\n",
        "#         temp_sentence = temp_sentence.replace(\".\", \" \") #Replace '.' by space\n",
        "#         temp_sentence_lemmered=[' '.join([lemmer.lemmatize(temp_sentence_words, 'v') for temp_sentence_words in temp_sentence.split(' ')])]\n",
        "#         temp_sentence_lemmered=[' '.join([lemmer.lemmatize(temp_sentence_words, 'a') for temp_sentence_words in temp_sentence_lemmered[0].split(' ')])]\n",
        "#         auto_revs_lemmered.append((temp_sentence_lemmered))\n",
        "    \n",
        "#     return auto_revs_lemmered\n",
        "\n",
        "# data_preprocessed = text_preprocess(auto_revs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OpNFphitQTf7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#importing CSV\n",
        "\n",
        "import csv\n",
        "\n",
        "with open(\"/content/drive/My Drive/COMP551-Project/data_preprocessed.csv\", 'r') as f:\n",
        "  reader = csv.reader(f)\n",
        "  data_preprocessed = list(reader)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ne4sV2wDZCr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# TF - IDF"
      ]
    },
    {
      "metadata": {
        "id": "fu-aiX189s6q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split data into train, validation and test sets\n",
        "\n",
        "# auto_revs_og_tf = np.array([str(i) for i in auto_revs_og])\n",
        "\n",
        "#auto_train, auto_test, auto_train_rates, auto_test_rates = train_test_split(auto_revs_og_tf, auto_rates, train_size=0.7, test_size=0.3, shuffle=True) # if want unprocessed data\n",
        "auto_train, auto_test, auto_train_rates, auto_test_rates = train_test_split(auto_revs, auto_rates, train_size=0.7, test_size=0.3, shuffle=True)\n",
        "auto_train, auto_valid, auto_train_rates, auto_valid_rates = train_test_split(auto_train, auto_train_rates, train_size=0.7, test_size=0.3)\n",
        "\n",
        "\n",
        "# perform TF-IDF and transform data into TF-IDF features\n",
        "\n",
        "tf_idf_vectorizer = TfidfVectorizer(stop_words='english', ngram_range=(1,1))             # substantiate\n",
        "vectors_train_idf = tf_idf_vectorizer.fit_transform(auto_train) # run tf-idf on training set\n",
        "#print(tf_idf_vectorizer.get_feature_names())                    # print words\n",
        "\n",
        "vectors_valid_idf = tf_idf_vectorizer.transform(auto_valid)\n",
        "vectors_test_idf = tf_idf_vectorizer.transform(auto_test)       # transform test set in tf-idf\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mUwyVIP0DCjM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Linear SVM"
      ]
    },
    {
      "metadata": {
        "id": "LiyxsuZg088a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### try tunning decision tree with data from baseline reproduction data\n",
        "\n",
        "# filename_train = \"drive/My Drive/COMP551-Project/automotive_filtered.txt\"\n",
        "# x_train_check, y_train_check = createDataset(filename_train)\n",
        "# #x_train_check = text_preprocess(x_train_check)\n",
        "\n",
        "# v_bin = CountVectorizer(analyzer=\"word\", binary=True, ngram_range = (1,3), token_pattern=u\"(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b\")\n",
        "# x_data_fold = v_bin.fit_transform(x_train_check.ravel())\n",
        "# y_data_fold = y_train_check\n",
        "\n",
        "# x_train_check, x_test_check, y_train_check, y_test_check = train_test_split(x_data_fold, y_data_fold, train_size=0.7, test_size=0.3, shuffle=True)\n",
        "# x_train_check, x_valid_check, y_train_check, y_valid_check = train_test_split(x_train_check, y_train_check, train_size=0.3, test_size=0.3, shuffle=True)\n",
        "\n",
        "# vectors_train_idf = x_train_check\n",
        "# auto_train_rates = y_train_check\n",
        "# vectors_valid_idf = x_valid_check\n",
        "# auto_valid_rates = y_valid_check\n",
        "# vectors_test_idf  = x_test_check\n",
        "# auto_test_rates = y_test_check\n",
        "\n",
        "### ### ###\n",
        "\n",
        "start_time=time.time()\n",
        "Cs=np.linspace(0,1,101)\n",
        "Cs=Cs[1:-1]\n",
        "lsvm_vld_f1s=[]\n",
        "cvs=[]\n",
        "\n",
        "\n",
        "for c in Cs:\n",
        "        #lsvm=OneVsRestClassifier(svm.LinearSVC(C=c), n_jobs=-2)\n",
        "        lsvm=svm.LinearSVC(C=c)\n",
        "        lsvm.fit(vectors_train_idf, auto_train_rates.ravel())\n",
        "        pred_vld=lsvm.predict(vectors_valid_idf)\n",
        "        lsvm_vld_f1s.append(f1_score(auto_valid_rates.ravel(), pred_vld, average='weighted'))\n",
        "        #k_fold_lsvm=cross_val_score(lsvm, vectors_train_idf, auto_train_rates.ravel(), cv=3)\n",
        "        #cvs.append(variation(k_fold_lsvm))\n",
        "    \n",
        "best_f1_lsvm=np.max(lsvm_vld_f1s)\n",
        "best_c=Cs[lsvm_vld_f1s.index(best_f1_lsvm)]\n",
        "#lsvm=OneVsRestClassifier(svm.LinearSVC(C=best_c), n_jobs=-2)\n",
        "lsvm=svm.LinearSVC(C=best_c)\n",
        "lsvm.fit(vectors_train_idf, auto_train_rates.ravel())\n",
        "\n",
        "pred_lsvm=lsvm.predict(vectors_test_idf)\n",
        "#k_fold_lsvm=cross_val_score(lsvm, x_data_fold, y_data_fold.ravel(), cv=10, scoring='f1_weighted')\n",
        "k_fold_lsvm=cross_val_score(lsvm, x_data_fold, y_data_fold.ravel(), cv=10, scoring='f1_weighted')\n",
        "\n",
        "duration=time.time() - start_time\n",
        "perf_lsvm=classification_report(auto_test_rates.ravel(), pred_lsvm)\n",
        "print(\"--- %s seconds ---\" % (duration))\n",
        "\n",
        "print(best_c)\n",
        "print(perf_lsvm)\n",
        "print('+/-',np.std(k_fold_lsvm))\n",
        "#print(duration)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XvX2-w8QDHkC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Kernelized SVM"
      ]
    },
    {
      "metadata": {
        "id": "ashYcqEV58m2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "uIQxiBvdW9Dv",
        "colab_type": "code",
        "outputId": "aa2d9d1b-13af-479a-9c03-87d4956d05a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "###  KERNELIZED SVM  ###\n",
        "\n",
        "start_time=time.time()\n",
        "Cs=np.linspace(0,1,51)   # C cost function, lecture 12 page 11\n",
        "Cs=Cs[1:-1]\n",
        "lsvm_vld_f1s=[]\n",
        "\n",
        "for c in Cs:\n",
        "        lsvm=OneVsRestClassifier(svm.SVC(C=c, kernel='rbf'), n_jobs=-2)\n",
        "        lsvm.fit(vectors_train_idf, auto_train_rates.ravel())\n",
        "        pred_vld=lsvm.predict(vectors_valid_idf)\n",
        "        lsvm_vld_f1s.append(f1_score(auto_valid_rates.ravel(), pred_vld, average='weighted'))\n",
        "    \n",
        "best_f1_lsvm=np.max(lsvm_vld_f1s)\n",
        "best_c=Cs[lsvm_vld_f1s.index(best_f1_lsvm)]\n",
        "lsvm=OneVsRestClassifier(svm.SVC(C=best_c, kernel='rbf'), n_jobs=-2)\n",
        "lsvm.fit(vectors_train_idf, auto_train_rates)\n",
        "\n",
        "pred_lsvm=lsvm.predict(vectors_test_idf)\n",
        "perf_lsvm=classification_report(auto_test_rates.ravel(), pred_lsvm)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
        "print(best_c)\n",
        "print(perf_lsvm)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--- 3046.403366804123 seconds ---\n",
            "0.04\n",
            "             precision    recall  f1-score   support\n",
            "\n",
            "   negative       0.65      0.74      0.69       329\n",
            "    neutral       0.75      0.40      0.52       339\n",
            "   positive       0.61      0.84      0.70       292\n",
            "\n",
            "avg / total       0.67      0.65      0.64       960\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "a-ivdaR1DLmi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gaussian Naive Bayes"
      ]
    },
    {
      "metadata": {
        "id": "KoufFVzfAAcE",
        "colab_type": "code",
        "outputId": "cf892da0-b273-46d6-ff5a-09abb7e1197a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "gnb=GaussianNB()\n",
        "gnb.fit(spm.toarray(vectors_train_idf), auto_train_rates.ravel())\n",
        "k_fold_gnb=cross_val_score(gnb, spm.toarray(vectors_train_idf), auto_train_rates.ravel(), cv=10, scoring='f1_weighted')\n",
        "\n",
        "pred_gnb=gnb.predict(spm.toarray(vectors_test_idf))\n",
        "perf_gnb = classification_report(auto_test_rates.ravel(), pred_gnb)\n",
        "duration = time.time() - start_time\n",
        "\n",
        "print(perf_gnb)\n",
        "#print(best_)\n",
        "print(np.mean(k_fold_gnb),'+/-',np.std(k_fold_gnb))\n",
        "print(duration)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             precision    recall  f1-score   support\n",
            "\n",
            "   negative       0.47      0.31      0.37       975\n",
            "    neutral       0.40      0.43      0.41       963\n",
            "   positive       0.43      0.55      0.48       942\n",
            "\n",
            "avg / total       0.43      0.43      0.42      2880\n",
            "\n",
            "0.422119887383691 +/- 0.017946853919394542\n",
            "17.19897985458374\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "SMdrDoJ8DPjL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "metadata": {
        "id": "iYJrfSHLmHrm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### try tunning decision tree with data from baseline reproduction\n",
        "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, train_size=0.7, test_size=0.3, shuffle=True)\n",
        "\n",
        "vectors_train_idf = x_train\n",
        "auto_train_rates = y_train\n",
        "vectors_valid_idf = x_valid\n",
        "auto_valid_rates = y_valid\n",
        "vectors_test_idf  = x_test\n",
        "auto_test_rates = y_test\n",
        "### ### ###\n",
        "\n",
        "start_time=time.time()\n",
        "\n",
        "max_depths = np.linspace(1, 32, 32, endpoint=True)\n",
        "min_samples_splits = np.linspace(0.1, 1.0, 10, endpoint=True)\n",
        "min_samples_leafs = np.linspace(0.1, 0.5, 5, endpoint=True)\n",
        "\n",
        "\n",
        "dt_f1s=[]\n",
        "all_dt_f1s=[]\n",
        "all_all_dt_f1s=[]\n",
        "all_all_all_dt_f1s=[]\n",
        "\n",
        "for leaf in min_samples_leafs:\n",
        "  for split in min_samples_splits:\n",
        "    for depth in max_depths:\n",
        "      dt = OneVsRestClassifier(tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=depth, min_samples_split=split, min_samples_leaf=leaf), n_jobs=-1)  # not c4.5 but CART (similar). choice of entropy based on https://stackoverflow.com/questions/34230063/can-we-choose-what-decision-tree-algorithm-to-use-in-sklearn\n",
        "      dt = dt.fit(vectors_train_idf, auto_train_rates)\n",
        "      pred_vld=dt.predict(vectors_valid_idf)\n",
        "      dt_f1s.append(f1_score(auto_valid_rates, pred_vld, average='weighted'))\n",
        "      \n",
        "    all_dt_f1s.append(dt_f1s)\n",
        "    dt_f1s=[]\n",
        "    \n",
        "  all_all_dt_f1s.append(all_dt_f1s)\n",
        "  all_dt_f1s=[]\n",
        "\n",
        "all_all_dt_f1s = np.array(all_all_dt_f1s)\n",
        "  \n",
        "best_f1_dt = np.max(all_all_dt_f1s)\n",
        "#best_depth=max_depths[dt_f1s.index(best_f1_dt)]\n",
        "best_idx = np.unravel_index(all_all_dt_f1s.argmax(), all_all_dt_f1s.shape)\n",
        "\n",
        "best_depth = max_depths[best_idx[2]]\n",
        "best_split = min_samples_splits[best_idx[1]]\n",
        "best_leaf = min_samples_leafs[best_idx[0]]\n",
        "\n",
        "dt = OneVsRestClassifier(tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=best_depth, min_samples_split=best_split, min_samples_leaf=best_leaf), n_jobs=-1)\n",
        "dt.fit(vectors_train_idf, auto_train_rates)\n",
        "k_fold_dt=cross_val_score(dt, vectors_train_idf, auto_train_rates.ravel(), cv=10, scoring='f1_weighted')\n",
        "\n",
        "pred_dt = dt.predict(vectors_test_idf)\n",
        "perf_dt = classification_report(auto_test_rates, pred_dt)\n",
        "\n",
        "duration=time.time() - start_time\n",
        "\n",
        "print(best_depth, best_split, best_leaf)\n",
        "print(perf_dt)\n",
        "print(np.mean(k_fold_dt),'+/-',np.std(k_fold_dt))\n",
        "print(duration)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ku7WDydld5h2",
        "colab_type": "code",
        "outputId": "005ed1db-516c-433e-b611-0646a5eec670",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 500, num = 4)]\n",
        "print(n_estimators)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[50, 200, 350, 500]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4p0VZYooDTIL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ]
    },
    {
      "metadata": {
        "id": "P6IO3CU8tdMa",
        "colab_type": "code",
        "outputId": "974a923a-7c09-400e-d693-81d0975855ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "cell_type": "code",
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 500, num = 4)]\n",
        "\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5, 10]\n",
        "\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True]\n",
        "\n",
        "grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "rf_random = GridSearchCV(rf, grid, cv = 3, verbose=1, n_jobs = -1)\n",
        "rf_random.fit(vectors_train_idf, auto_train_rates.ravel())\n",
        "#k_fold_rf=cross_val_score(rf_random, vectors_train_idf, auto_train_rates.ravel(), cv=10, scoring='f1_weighted')\n",
        "k_fold_lsvm=cross_val_score(rf_random, x_data_fold, y_data_fold.ravel(), cv=10, scoring='f1_weighted')\n",
        "\n",
        "#rf.fit(vectors_train_idf, auto_train_rates.ravel())\n",
        "\n",
        "#pred_rf = rf.predict(vectors_test_idf)\n",
        "#perf_rf = classification_report(auto_test_rates, pred_rf)\n",
        "#print(perf_rf)\n",
        "duration = time.time() - start_time\n",
        "\n",
        "pred_rf = rf_random.predict(vectors_test_idf)\n",
        "perf_rf = classification_report(auto_test_rates, pred_rf)\n",
        "\n",
        "print(rf_random.best_params_)\n",
        "print(perf_rf)\n",
        "print(np.mean(k_fold_rf),'+/-',np.std(k_fold_rf))\n",
        "print(duration)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 864 candidates, totalling 2592 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  46 tasks      | elapsed:  1.0min\n",
            "[Parallel(n_jobs=-1)]: Done 196 tasks      | elapsed:  4.1min\n",
            "[Parallel(n_jobs=-1)]: Done 446 tasks      | elapsed: 12.7min\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed: 30.9min\n",
            "[Parallel(n_jobs=-1)]: Done 1246 tasks      | elapsed: 61.5min\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "OLd_lMRtIJrm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(rf_random.best_params_)\n",
        "print(perf_rf)\n",
        "print(np.mean(k_fold_rf),'+/-',np.std(k_fold_rf))\n",
        "print(duration)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
